# HAM10000 supervised fine-tuning config (multiclass)
seed: 42

runs_root: /scratch/ali95/ham_runs
name: ham10000_blip2opt2p7b

run:
  output_root: /scratch/ali95/blip2_interleaved_project/runs
  exp_name: ham10000_finetune_blip2

data:
  images_dir: /home/ali95/LAVIS/datasets/HAM10000/images
  meta_csv: /home/ali95/LAVIS/datasets/HAM10000/HAM10000_metadata.csv
  splits_dir: /home/ali95/LAVIS/datasets/HAM10000/splits
  

  # ? ADD THESE (match what run.py expects)
  train_csv: /home/ali95/LAVIS/datasets/HAM10000/splits/ham_train.csv
  val_csv:   /home/ali95/LAVIS/datasets/HAM10000/splits/ham_val.csv
  test_csv:  /home/ali95/LAVIS/datasets/HAM10000/splits/ham_test.csv

  img_ext: .jpg
  img_size: 224
  num_workers: 4
  


model:
  name: blip2_classifier
  blip2:
    model_type: pretrain_opt2.7b
    device: cuda
    #lavis_name: blip2_opt
    #freeze_vision: true
    train_qformer: false
    #pooling: cls
    #dropout: 0.0


//model:
//  lavis_name: blip2_feature_extractor
//  model_type: pretrain_vitL
//  device: cuda
//  freeze_vision: true
//  pooling: cls
//  dropout: 0.0
    #train_qformer: false

train:
  epochs: 15
  batch_size: 32
  accum_steps: 1
  amp: true
  grad_clip_norm: 1.0

  optimizer: adamw
  lr: 2.0e-4
  weight_decay: 5.0e-2
  betas: [0.9, 0.999]
  eps: 1.0e-8

  warmup_ratio: 0.05
  scheduler: cosine

  label_smoothing: 0.1

  class_weights: true
  sampler: weighted
  # unfreeze_vision_epoch: 5

eval:
  batch_size: 64
  metrics: ["acc","balanced_acc","f1_macro","f1_weighted","auc_ovr"]
  save_best: "f1_macro"

wandb:
  enabled: true
  project: ham10000
  mode: offline
  tags: ["finetune","multiclass","blip2"]
